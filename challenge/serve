#!/usr/bin/env python

"""
This file implements the prediction service shell. You don't necessarily
need to modify it for various algorithms. It starts nginx and
gunicorn with the correct configurations and then simply waits
until gunicorn exits.
"""

# Standar library
import os
import sys
import signal
import subprocess
import multiprocessing

# Constants
cpu_count = multiprocessing.cpu_count()
model_server_timeout = os.environ.get('MODEL_SERVER_TIMEOUT', 60)
model_server_workers = int(os.environ.get('MODEL_SERVER_WORKERS', cpu_count))


def sigterm_handler(nginx_pid, gunicorn_pid):
    try:
        os.kill(nginx_pid, signal.SIGQUIT)
    except OSError:
        pass
    try:
        os.kill(gunicorn_pid, signal.SIGTERM)
    except OSError:
        pass
    sys.exit(0)


def start_server():
    print('Starting Model Endpoint Server...')
    print(
        f'Inference server started with {model_server_workers} workers.')

    # link the log streams to stdout/err so they will be
    # logged to the container logs
    subprocess.check_call([
        'ln', '-sf', '/dev/stdout', '/var/log/nginx/access.log'])
    subprocess.check_call([
        'ln', '-sf', '/dev/stderr', '/var/log/nginx/error.log'])

    nginx = subprocess.Popen([
        'nginx', '-c', '/opt/ml/pkg/nginx.conf'
    ])

    gunicorn = subprocess.Popen([
        'gunicorn',
        '--timeout', str(model_server_timeout),
        '-p', '8080',
        '-k', 'uvicorn.workers.UvicornWorker',
        '-b', 'unix:/tmp/gunicorn.sock',
        '-w', str(model_server_workers),
        'challenge.api:app'
    ])

    signal.signal(
        signal.SIGTERM,
        lambda a, b: sigterm_handler(nginx.pid, gunicorn.pid)
    )
    # If either subprocess exits, so do we.W
    pids = set([nginx.pid, gunicorn.pid])
    while True:
        pid, _ = os.wait()
        if pid in pids:
            break

    sigterm_handler(nginx.pid, gunicorn.pid)
    print('Inference server exiting')


if __name__ == '__main__':
    start_server()
